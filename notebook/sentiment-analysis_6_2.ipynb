{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6.2: Preparing Data for Final Project\n",
    "## Team 2: Leonid Shpaner, Jose Luis Estrada, Christopher Robinson\n",
    "### This notebook implements a text mining sentiment analysis project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvPmfHEMcsh-"
   },
   "source": [
    "## First we fetch the data from google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1715,
     "status": "ok",
     "timestamp": 1655653364139,
     "user": {
      "displayName": "Juan Huerta",
      "userId": "13920624898716497771"
     },
     "user_tz": 300
    },
    "id": "_vOp1JWVDqFg",
    "outputId": "62a3773a-88f1-4094-f483-e946514e3742"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lshpaner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfATH8s2DqFi"
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=10rDgl5zAvUdVgSoVngHfwJnf8I1tdpZi\n",
      "To: c:\\Users\\lshpaner\\Documents\\Github Repositories\\msads509_final_project\\twitter_emotions\\notebook\\train_data.csv\n",
      "100%|██████████| 239M/239M [03:09<00:00, 1.26MB/s] \n",
      "\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=10qeDcgwdJC76Nv5cCj6WsUYjD6846fEL\n",
      "To: c:\\Users\\lshpaner\\Documents\\Github Repositories\\msads509_final_project\\twitter_emotions\\notebook\\test_data.csv\n",
      "100%|██████████| 74.3k/74.3k [00:00<00:00, 436kB/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import subprocess\n",
    "\n",
    "def download_gdrive(id, print_stout=True):\n",
    "    coomand = 'gdown https://drive.google.com/uc?id={}'.format(id)\n",
    "    returned_value = subprocess.run(coomand, shell=True, stdout=subprocess.PIPE, \n",
    "    stderr=subprocess.STDOUT)\n",
    "    if print_stout: print(returned_value.stdout.decode(\"utf-8\"))\n",
    "    else: print(\"Download Complete\")\n",
    "\n",
    "train_data = download_gdrive(\"10rDgl5zAvUdVgSoVngHfwJnf8I1tdpZi\", print_stout=True)\n",
    "test_data = download_gdrive(\"10qeDcgwdJC76Nv5cCj6WsUYjD6846fEL\", print_stout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['polarity', 'tweetid', 'date', 'query_name', 'user', 'text']\n",
    "dftrain = pd.read_csv('train_data.csv',\n",
    "                      header = None,\n",
    "                      encoding ='ISO-8859-1')\n",
    "dftest = pd.read_csv('test_data.csv',\n",
    "                     header = None,\n",
    "                     encoding ='ISO-8859-1')\n",
    "dftrain.columns = columns\n",
    "dftest.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1655653367655,
     "user": {
      "displayName": "Juan Huerta",
      "userId": "13920624898716497771"
     },
     "user_tz": 300
    },
    "id": "VuHyYB5PXdGD",
    "outputId": "1d30867e-ca40-4758-bbce-c613d42af262"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>date</th>\n",
       "      <th>query_name</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>676838</th>\n",
       "      <td>0</td>\n",
       "      <td>2248511881</td>\n",
       "      <td>Fri Jun 19 20:44:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jawnahthin</td>\n",
       "      <td>I am exhausted... and I think I left my brain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703009</th>\n",
       "      <td>0</td>\n",
       "      <td>2255579257</td>\n",
       "      <td>Sat Jun 20 11:28:32 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>wendy_munro</td>\n",
       "      <td>@monkeycoco They are all repeats here now.  No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620728</th>\n",
       "      <td>0</td>\n",
       "      <td>2228512707</td>\n",
       "      <td>Thu Jun 18 14:31:59 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>miriamjlee</td>\n",
       "      <td>Had a lovely nap!~ time to get ready for psych...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204185</th>\n",
       "      <td>0</td>\n",
       "      <td>1972555803</td>\n",
       "      <td>Sat May 30 09:26:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AKLSweets41</td>\n",
       "      <td>is mad about her new dress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011756</th>\n",
       "      <td>4</td>\n",
       "      <td>1881136628</td>\n",
       "      <td>Fri May 22 03:43:24 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>camden_girl</td>\n",
       "      <td>@mattconfusion hi teo! yes here I am...quite s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886151</th>\n",
       "      <td>4</td>\n",
       "      <td>1686661949</td>\n",
       "      <td>Sun May 03 06:19:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sanniu</td>\n",
       "      <td>@allmae yea it is ! can't wait to c next episode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>0</td>\n",
       "      <td>1468390805</td>\n",
       "      <td>Tue Apr 07 01:22:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>dionbp</td>\n",
       "      <td>Morning!! I'm baggered! Been the gym then off ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509077</th>\n",
       "      <td>4</td>\n",
       "      <td>2174752210</td>\n",
       "      <td>Sun Jun 14 23:14:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Ashhh_</td>\n",
       "      <td>today i brought the coolest Hannah Montana nec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260870</th>\n",
       "      <td>4</td>\n",
       "      <td>1998455170</td>\n",
       "      <td>Mon Jun 01 18:34:36 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Sicklillovesong</td>\n",
       "      <td>@RobCusella This year...I want summer like now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676515</th>\n",
       "      <td>0</td>\n",
       "      <td>2248421087</td>\n",
       "      <td>Fri Jun 19 20:35:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>DanaJ12</td>\n",
       "      <td>has a broken car. An extremely broken car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity     tweetid                          date query_name  \\\n",
       "676838          0  2248511881  Fri Jun 19 20:44:31 PDT 2009   NO_QUERY   \n",
       "703009          0  2255579257  Sat Jun 20 11:28:32 PDT 2009   NO_QUERY   \n",
       "620728          0  2228512707  Thu Jun 18 14:31:59 PDT 2009   NO_QUERY   \n",
       "204185          0  1972555803  Sat May 30 09:26:48 PDT 2009   NO_QUERY   \n",
       "1011756         4  1881136628  Fri May 22 03:43:24 PDT 2009   NO_QUERY   \n",
       "886151          4  1686661949  Sun May 03 06:19:11 PDT 2009   NO_QUERY   \n",
       "2507            0  1468390805  Tue Apr 07 01:22:48 PDT 2009   NO_QUERY   \n",
       "1509077         4  2174752210  Sun Jun 14 23:14:04 PDT 2009   NO_QUERY   \n",
       "1260870         4  1998455170  Mon Jun 01 18:34:36 PDT 2009   NO_QUERY   \n",
       "676515          0  2248421087  Fri Jun 19 20:35:51 PDT 2009   NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "676838        jawnahthin  I am exhausted... and I think I left my brain ...  \n",
       "703009       wendy_munro  @monkeycoco They are all repeats here now.  No...  \n",
       "620728        miriamjlee  Had a lovely nap!~ time to get ready for psych...  \n",
       "204185       AKLSweets41                        is mad about her new dress   \n",
       "1011756      camden_girl  @mattconfusion hi teo! yes here I am...quite s...  \n",
       "886151            sanniu  @allmae yea it is ! can't wait to c next episode   \n",
       "2507              dionbp  Morning!! I'm baggered! Been the gym then off ...  \n",
       "1509077           Ashhh_  today i brought the coolest Hannah Montana nec...  \n",
       "1260870  Sicklillovesong    @RobCusella This year...I want summer like now   \n",
       "676515           DanaJ12         has a broken car. An extremely broken car   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KO0s_J8bDqFi"
   },
   "source": [
    "### Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1655653367656,
     "user": {
      "displayName": "Juan Huerta",
      "userId": "13920624898716497771"
     },
     "user_tz": 300
    },
    "id": "pKR02fmWDqFi"
   },
   "outputs": [],
   "source": [
    "user_pat = '(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)'\n",
    "http_pat = '(https?:\\/\\/(?:www\\.|(?!www))[^\\s\\.]+\\.[^\\s]{2,}|www\\.[^\\s]+\\.[^\\s]{2,})'\n",
    "repeat_pat, repeat_repl = \"(.)\\\\1\\\\1+\",'\\\\1\\\\1'\n",
    "    \n",
    "def transform(X): \n",
    "    pp_text = X\n",
    "    pp_text = pp_text.str.replace(pat = user_pat, repl = 'USERNAME')\n",
    "    pp_text = pp_text.str.replace(pat = http_pat, repl = 'URL')\n",
    "    pp_text.str.replace(pat = repeat_pat, repl = repeat_repl)\n",
    "    return pp_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srlyyoLSFBUu"
   },
   "source": [
    "## Descriptive Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1655653367656,
     "user": {
      "displayName": "Juan Huerta",
      "userId": "13920624898716497771"
     },
     "user_tz": 300
    },
    "id": "C6TVthPhG6jm"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "def descriptive_stats(tokens, top_num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \"\"\"\n",
    "\n",
    "    # Fill in the correct values here. \n",
    "    \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    lexical_diversity = num_unique_tokens/num_tokens\n",
    "    num_characters = len(\"\".join(tokens))\n",
    "\n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        \n",
    "        # use a list comprehension on a set to exclude repeating\n",
    "        # tokens and empty strings\n",
    "        unique_tokens = [token for token in set(tokens) if token]\n",
    "        # use a unique tokens to check frequency of tokens in \n",
    "        # original list\n",
    "        counts = [tokens.count(token) for token in unique_tokens]\n",
    "        result = []\n",
    "        \n",
    "        # iterate over the range of tokens to locate and find the\n",
    "        # maximum count, then mutate both unique_tokens and counts\n",
    "        # based on the associated position\n",
    "        for _ in range(top_num_tokens):\n",
    "            max_count = max(counts)\n",
    "            max_count_pos = counts.index(max_count)\n",
    "            most_common = unique_tokens.pop(max_count_pos)\n",
    "            result.append(most_common)\n",
    "            counts.pop(max_count_pos)\n",
    "\n",
    "        print(result)\n",
    "            \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 453789,
     "status": "ok",
     "timestamp": 1655653821439,
     "user": {
      "displayName": "Juan Huerta",
      "userId": "13920624898716497771"
     },
     "user_tz": 300
    },
    "id": "CvS2eYNNFFWg",
    "outputId": "373375b6-8028-4ee5-e537-a0cf4ab0a4db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESCRIPTIVE STATS ON TEXT: \n",
      "There are 158217 tokens in the data.\n",
      "There are 17778 unique tokens in the data.\n",
      "There are 583544 characters in the data.\n",
      "The lexical diversity is 0.112 in the data.\n",
      "['!', '.', 'USERNAME', 'I', 'to']\n",
      "\n",
      "\n",
      "DESCRIPTIVE STATS ON SENTIMENT POLARITY:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1.600000e+06\n",
       "mean     2.000000e+00\n",
       "std      2.000001e+00\n",
       "min      0.000000e+00\n",
       "25%      0.000000e+00\n",
       "50%      2.000000e+00\n",
       "75%      4.000000e+00\n",
       "max      4.000000e+00\n",
       "Name: polarity, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#PRE-Process step\n",
    "#feel free to add more items to the pre-processing step\n",
    "dftrain['text'] = transform(dftrain['text'])\n",
    "##word_tokenize \n",
    "dftrain['text'] = dftrain.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "\n",
    "print(\"DESCRIPTIVE STATS ON TEXT: \")\n",
    "all = []\n",
    "#on 10k data\n",
    "for li in dftrain['text'].sample(10000).iteritems(): all += li[1]\n",
    "#on all data  \n",
    "# for li in dftrain['text'].iteritems(): all += li[1]\n",
    "\n",
    "#feel free to add more items to analyze in descriptive_stats\n",
    "descriptive_stats(all, verbose=True)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"DESCRIPTIVE STATS ON SENTIMENT POLARITY:\")\n",
    "dftrain['polarity'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yrA6HZiDqFj"
   },
   "source": [
    "### TODO: Train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1655653821439,
     "user": {
      "displayName": "Juan Huerta",
      "userId": "13920624898716497771"
     },
     "user_tz": 300
    },
    "id": "V0ADNRDPDqFk"
   },
   "outputs": [],
   "source": [
    "Xtest, ytest = dftest.text[dftest.polarity!=2], dftest.polarity[dftest.polarity!=2]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sentiment-analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b61bb2d7a674932f79ec86662a4165aa288473e6e47a51dc8e3ab9a9cd94608c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
